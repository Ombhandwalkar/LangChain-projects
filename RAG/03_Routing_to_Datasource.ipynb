{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwh22fw_uDVe"
      },
      "source": [
        "# RAG From Scratch: Routing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcbnf7rpuCAv",
        "outputId": "a597c116-a0fd-4b0b-9812-ade4d4901594"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain-google-genai langchainhub chromadb langchain youtube-transcript-api pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enY4ETv6uzun"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2']='true'\n",
        "os.environ['LANGCHAIN_ENDPOINT']='https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY']='LANGCHAIN_API_KEY'\n",
        "os.environ['GOOGLE_API_KEY']='GOOGLE_API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8MZN5Bkvo-a"
      },
      "source": [
        "# Logical Routing-\n",
        "Routing is based on what type of question is asked to LLM. Accoding to the question LLM decides which data source will relevant to fetch relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lIj_6W1wvlRc"
      },
      "outputs": [],
      "source": [
        "from typing import Literal # Litreal is the type of module which is used to restirct a variable to a specific set of values.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # ChatPrompTemplate - Is specifically designed for Chat based LLM\n",
        "from langchain_core.pydantic_v1 import BaseModel,Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Data Model-\n",
        "# pydantic_model that validate the LLM's output. The datasource field can only be one of three values.\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm=ChatGoogleGenerativeAI(model='gemini-1.5-flash',temperature=0)\n",
        "structured_llm=llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system=\"\"\"\n",
        "    You are a expert at routing user question to the appropriate data source.\n",
        "    Based on programming language th question is referring to, route it to relevant data source.\n",
        "\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          ('system',system),\n",
        "          ('human',\"{question}\")\n",
        "      ]\n",
        ")\n",
        "\n",
        "# Define  Router\n",
        "router = prompt | structured_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PLVwVYFlBhNQ"
      },
      "outputs": [],
      "source": [
        "question=\"\"\"\n",
        "      Why does not following code work:\n",
        "      from langchain_core.prompts import ChatPromptTemplate\n",
        "      prompt=ChatPromptTemplate.from_messages([\"human\",\"speak in {language}\"])\n",
        "      prompt.invoke('French')\n",
        "    \"\"\"\n",
        "\n",
        "result=router.invoke({'question':question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8dSrMcclCOEv",
        "outputId": "acd5b94f-748b-4c93-8e4d-dbc24d43c018"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'python_docs'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.datasource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r-qoXiuWCPOG"
      },
      "outputs": [],
      "source": [
        "def choose_route(result):\n",
        "  if 'python_docs' in result.datasource.lower():\n",
        "    return 'chain for python_docs'\n",
        "  elif 'js_docs' in result.datasource.lower():\n",
        "    return 'chain for js_docs'\n",
        "  else:\n",
        "    return 'chain for golang_docs'\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yg-eV2h7ERxM",
        "outputId": "edb4317a-9de3-4fdc-9a18-f7097c2b65b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chain for python_docs'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7BC9-mSF4kq"
      },
      "source": [
        "# Semantic Routing\n",
        "Accoding to user question LLM decides the which prompt is best suitable to fetch relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlGYYKONFBbo",
        "outputId": "9bfa6d97-f79d-4b37-b45e-5ee466fdeeb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Physics\n",
            "A black hole is a region of spacetime with gravity so strong that nothing, not even light, can escape.  It's formed when a massive star collapses at the end of its life.\n"
          ]
        }
      ],
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda,RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Physics teacher Prompt\n",
        "physics_template=\"\"\"\n",
        "      You are very smart physics professor.\\\n",
        "      You are great at answering questions about physics in very concise and easy to understand manner.\\\n",
        "      When you don't know the answer to a question you admit that you don't know.\n",
        "      Here is the question:{query}\n",
        "      \"\"\"\n",
        "# Maths Teacher Prompt\n",
        "math_template=\"\"\"\n",
        "      You are a very good mathematician. You are great at ansering math questions \\\n",
        "      You are very good because you are able to break down hard problem into their component parts,\\\n",
        "      answer the component parts, and put them together to answer the border question.\n",
        "      Here is the question:{query}\n",
        "\"\"\"\n",
        "\n",
        "# Embed Prompt\n",
        "Embeddings=GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
        "prompt_template=[physics_template,math_template]\n",
        "prompt_embeddings=Embeddings.embed_documents(prompt_template)\n",
        "\n",
        "\n",
        "# Route question to the Prompt\n",
        "def prompt_router(input):\n",
        "  # Embedded question\n",
        "  query_embedding= Embeddings.embed_query(input['query'])\n",
        "  # Compute Similarity\n",
        "  similarity=cosine_similarity([query_embedding],prompt_embeddings)[0]\n",
        "  most_similar=prompt_template[similarity.argmax()]\n",
        "  # Chosen prompt\n",
        "  print('Using math' if most_similar==math_template else 'Using Physics')\n",
        "  return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "\n",
        "chain=(\n",
        "    {'query':RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke('What is black hole'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fdvFFq9SUaN"
      },
      "source": [
        "# Query Structuring for Metadata Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSn-X1AYKqv9",
        "outputId": "62f4f308-2405-48c4-bd34-50e98540aac2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'pbAd8O1Lvm4'}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "docs=YoutubeLoader.from_youtube_url('https://www.youtube.com/watch?v=pbAd8O1Lvm4').load()\n",
        "docs[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UDYFObYRTqKa"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from typing import Literal, Optional, Tuple\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class TutorialSearch(BaseModel):\n",
        "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
        "\n",
        "    content_search: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to video transcripts.\",\n",
        "    )\n",
        "    title_search: str = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"Alternate version of the content search query to apply to video titles. \"\n",
        "            \"Should be succinct and only include key words that could be in a video \"\n",
        "            \"title.\"\n",
        "        ),\n",
        "    )\n",
        "    min_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    earliest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    latest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    min_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "\n",
        "    def pretty_print(self) -> None:\n",
        "        for field in self.__fields__:\n",
        "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
        "                self.__fields__[field], \"default\", None\n",
        "            ):\n",
        "                print(f\"{field}: {getattr(self, field)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mgDognbsYi-o"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a database query optimized to retrieve the most relevant results.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(TutorialSearch)\n",
        "query_analyzer = prompt | structured_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4T1WFmWYrV2",
        "outputId": "bed997b2-a91e-4cb5-e8fb-94afd655db99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: rag from scratch\n",
            "title_search: rag from scratch\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj1nEHpCYtTR",
        "outputId": "118645ec-a002-45b1-d505-98098453aa85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: chat langchain\n",
            "title_search: chat langchain\n",
            "earliest_publish_date: 2023-01-01\n",
            "latest_publish_date: 2024-01-01\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke(\n",
        "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
        ").pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo93IsbMYv65",
        "outputId": "e2f76bd6-c975-4d37-d4ac-bcc0ed57013b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: chat langchain\n",
            "title_search: chat langchain\n",
            "latest_publish_date: 1970-01-01\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke(\n",
        "    {\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\"}\n",
        ").pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kophetGpYxtd",
        "outputId": "23c78dd5-633d-4e9c-d744-526bf26b1c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content_search: how to use multi-modal models in an agent\n",
            "title_search: multi-modal agent\n",
            "max_length_sec: 300\n"
          ]
        }
      ],
      "source": [
        "query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
        "    }\n",
        ").pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjxPhwo7YzGn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As AI language models become more sophisticated, the quality of prompts used to interact with them becomes increasingly important. Optimized prompts can lead to more accurate, relevant, and useful responses, enhancing the overall performance of AI applications. This tutorial aims to equip learners with practical techniques to systematically improve their prompts.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **A/B Testing Prompts**: A method to compare the effectiveness of different prompt variations.\n",
        "2. **Iterative Refinement**: A strategy for gradually improving prompts based on feedback and results.\n",
        "3. **Performance Metrics**: Ways to measure and compare the quality of responses from different prompts.\n",
        "4. **Practical Implementation**: Hands-on examples using OpenAI's GPT model and LangChain.\n",
        "\n",
        "## Method Details\n",
        "\n",
        "1. **Setup**: We'll start by setting up our environment with the necessary libraries and API keys.\n",
        "\n",
        "2. **A/B Testing**:\n",
        "   - Define multiple versions of a prompt\n",
        "   - Generate responses for each version\n",
        "   - Compare results using predefined metrics\n",
        "\n",
        "3. **Iterative Refinement**:\n",
        "   - Start with an initial prompt\n",
        "   - Generate responses and evaluate\n",
        "   - Identify areas for improvement\n",
        "   - Refine the prompt based on insights\n",
        "   - Repeat the process to continuously enhance the prompt\n",
        "\n",
        "4. **Performance Evaluation**:\n",
        "   - Define relevant metrics (e.g., relevance, specificity, coherence)\n",
        "   - Implement scoring functions\n",
        "   - Compare scores across different prompt versions\n"
      ],
      "metadata": {
        "id": "14pwwj2koMT0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tYoGjaGveWeD",
        "outputId": "f038731a-71cf-41d1-cca5-933598b27738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.4-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "44446bb313b14834b27da1dc1c4ab300"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import numpy as np\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=''\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
        "\n",
        "def generate_response(prompt):\n",
        "  \"\"\"Generate a response using the language model.\n",
        "\n",
        "  Args:\n",
        "      prompt (str): The input prompt.\n",
        "\n",
        "  Returns:\n",
        "      str: The generated response.\n",
        "  \"\"\"\n",
        "  return llm.invoke(prompt).content"
      ],
      "metadata": {
        "id": "qgztRV1yo3sD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A/B Testing Prompt\n",
        "We will ask LLM to generate  answers from 2 prompts, Also ask to rate them on the scale of 1 to 10. will ask - Tell which prompt is best for relevant answer."
      ],
      "metadata": {
        "id": "qxYvGB27ppMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt variations\n",
        "prompt_a = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Explain {topic} in simple terms.\"\n",
        ")\n",
        "\n",
        "prompt_b = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Provide a beginner-friendly explanation of {topic}, including key concepts and an example.\"\n",
        ")\n",
        "\n",
        "# Updated function to evaluate response quality\n",
        "def evaluate_response(response, criteria):\n",
        "    \"\"\"Evaluate the quality of a response based on given criteria.\n",
        "\n",
        "    Args:\n",
        "        response (str): The generated response.\n",
        "        criteria (list): List of criteria to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        float: The average score across all criteria.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for criterion in criteria:\n",
        "        print(f\"Evaluating response based on {criterion}...\")\n",
        "        prompt = f\"On a scale of 1-10, rate the following response on {criterion}. Start your response with the numeric score:\\n\\n{response}\"\n",
        "        response = generate_response(prompt)\n",
        "        # show 50 characters of the response\n",
        "        # Use regex to find the first number in the response\n",
        "        score_match = re.search(r'\\d+', response)\n",
        "        if score_match:\n",
        "            score = int(score_match.group())\n",
        "            scores.append(min(score, 10))  # Ensure score is not greater than 10\n",
        "        else:\n",
        "            print(f\"Warning: Could not extract numeric score for {criterion}. Using default score of 5.\")\n",
        "            scores.append(5)  # Default score if no number is found\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Perform A/B test\n",
        "topic = \"machine learning\"\n",
        "response_a = generate_response(prompt_a.format(topic=topic))\n",
        "response_b = generate_response(prompt_b.format(topic=topic))\n",
        "\n",
        "criteria = [\"clarity\", \"informativeness\", \"engagement\"]\n",
        "score_a = evaluate_response(response_a, criteria)\n",
        "score_b = evaluate_response(response_b, criteria)\n",
        "\n",
        "print(f\"Prompt A score: {score_a:.2f}\")\n",
        "print(f\"Prompt B score: {score_b:.2f}\")\n",
        "print(f\"Winning prompt: {'A' if score_a > score_b else 'B'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMGoa7qpnMM",
        "outputId": "f237110a-8ef7-4cc6-e377-07035d87cea7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating response based on clarity...\n",
            "Evaluating response based on informativeness...\n",
            "Evaluating response based on engagement...\n",
            "Evaluating response based on clarity...\n",
            "Evaluating response based on informativeness...\n",
            "Evaluating response based on engagement...\n",
            "Prompt A score: 9.00\n",
            "Prompt B score: 9.00\n",
            "Winning prompt: B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterative Refinment\n",
        "* Will ask LLM to generate answers on two different prompts\n",
        "* Will ask to rate them on scale 1 to 10\n",
        "* Ask which one is the best prompt for relevant output\n",
        "* Will ask LLM to generate feedback and imporvement for Best prompt's output.\n",
        "* Will ask LLM to according to feedback refine the prompt and generate the output."
      ],
      "metadata": {
        "id": "4M1PxSk3so_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_prompt(initial_prompt, topic, iterations=3):\n",
        "    \"\"\"Refine a prompt through multiple iterations.\n",
        "\n",
        "    Args:\n",
        "        initial_prompt (PromptTemplate): The starting prompt template.\n",
        "        topic (str): The topic to explain.\n",
        "        iterations (int): Number of refinement iterations.\n",
        "\n",
        "    Returns:\n",
        "        PromptTemplate: The final refined prompt template.\n",
        "    \"\"\"\n",
        "    current_prompt = initial_prompt\n",
        "    for i in range(iterations):\n",
        "        try:\n",
        "            response = generate_response(current_prompt.format(topic=topic))\n",
        "        except KeyError as e:\n",
        "            print(f\"Error in iteration {i+1}: Missing key {e}. Adjusting prompt...\")\n",
        "            # Remove the problematic placeholder\n",
        "            current_prompt.template = current_prompt.template.replace(f\"{{{e.args[0]}}}\", \"relevant example\")\n",
        "            response = generate_response(current_prompt.format(topic=topic))\n",
        "\n",
        "        # Generate feedback and suggestions for improvement\n",
        "        feedback_prompt = f\"Analyze the following explanation of {topic} and suggest improvements to the prompt that generated it:\\n\\n{response}\"\n",
        "        feedback = generate_response(feedback_prompt)\n",
        "\n",
        "        # Use the feedback to refine the prompt\n",
        "        refine_prompt = f\"Based on this feedback: '{feedback}', improve the following prompt template. Ensure to only use the variable {{topic}} in your template:\\n\\n{current_prompt.template}\"\n",
        "        refined_template = generate_response(refine_prompt)\n",
        "\n",
        "        current_prompt = PromptTemplate(\n",
        "            input_variables=[\"topic\"],\n",
        "            template=refined_template\n",
        "        )\n",
        "\n",
        "        print(f\"Iteration {i+1} prompt: {current_prompt.template}\")\n",
        "\n",
        "    return current_prompt\n",
        "\n",
        "# Perform A/B test\n",
        "topic = \"machine learning\"\n",
        "response_a = generate_response(prompt_a.format(topic=topic))\n",
        "response_b = generate_response(prompt_b.format(topic=topic))\n",
        "\n",
        "criteria = [\"clarity\", \"informativeness\", \"engagement\"]\n",
        "score_a = evaluate_response(response_a, criteria)\n",
        "score_b = evaluate_response(response_b, criteria)\n",
        "\n",
        "print(f\"Prompt A score: {score_a:.2f}\")\n",
        "print(f\"Prompt B score: {score_b:.2f}\")\n",
        "print(f\"Winning prompt: {'A' if score_a > score_b else 'B'}\")\n",
        "\n",
        "# Start with the winning prompt from A/B testing\n",
        "initial_prompt = prompt_b if score_b > score_a else prompt_a\n",
        "refined_prompt = refine_prompt(initial_prompt, \"machine learning\")\n",
        "\n",
        "print(\"\\nFinal refined prompt:\")\n",
        "print(refined_prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEUo3RK9sRbD",
        "outputId": "221c2e89-cfaf-4c02-e738-6e00fc7e10b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating response based on clarity...\n",
            "Evaluating response based on informativeness...\n",
            "Evaluating response based on engagement...\n",
            "Evaluating response based on clarity...\n",
            "Evaluating response based on informativeness...\n",
            "Evaluating response based on engagement...\n",
            "Prompt A score: 9.00\n",
            "Prompt B score: 9.00\n",
            "Winning prompt: B\n",
            "Iteration 1 prompt: Here are a few improved prompt templates based on the feedback, using only the `{topic}` variable:\n",
            "\n",
            "**Option 1 (More detailed):**\n",
            "\n",
            "Explain {topic}, including its core principles, different types, the role of training data, the importance of generalization, and potential challenges.  Use a simple analogy to illustrate the process.\n",
            "\n",
            "**Option 2 (Focus on a specific aspect – suitable if {topic} is a specific type of machine learning):**\n",
            "\n",
            "Explain {topic}, detailing its process, the role of training data, model training, model evaluation, and the concept of generalization.  Discuss potential challenges such as overfitting and bias.\n",
            "\n",
            "**Option 3 (Comparative approach – suitable if {topic} encompasses multiple sub-topics):**\n",
            "\n",
            "Compare and contrast the different aspects of {topic}, providing a simple example for each and explaining their differences in approach, data requirements, and goals.\n",
            "\n",
            "\n",
            "These options provide more structure and guidance than the original prompt, encouraging a more comprehensive and accurate response.  They address the weaknesses identified in the feedback by explicitly requesting information on crucial concepts like different types, training data, generalization, and potential challenges.\n",
            "Iteration 2 prompt: Here are improved prompt templates, incorporating the feedback and using only the `{topic}` variable:\n",
            "\n",
            "\n",
            "**Option 1 (Broad, with analogy and length constraint):**\n",
            "\n",
            "Explain {topic} to a non-technical audience in under 500 words, using a simple analogy.  Include its core principles and potential applications.\n",
            "\n",
            "\n",
            "**Option 2 (Specific, with focus on process and challenges):**\n",
            "\n",
            "Explain the process of {topic}, including data requirements, model training, evaluation metrics, and common challenges like overfitting and bias.\n",
            "\n",
            "\n",
            "**Option 3 (Comparative, with specified output):**\n",
            "\n",
            "Compare and contrast at least three key approaches within {topic}, highlighting their differences in methodology, data needs, and typical applications.  Present your comparison in a table format.\n",
            "\n",
            "\n",
            "**Option 4 (Technical, with dataset example -  suitable if {topic} involves a specific dataset or algorithm):**\n",
            "\n",
            "Explain {topic} using the MNIST dataset as an example.  Detail the mathematical foundations, training process, and evaluation metrics.  Focus on generalization and potential areas of improvement.\n",
            "\n",
            "\n",
            "These options address the feedback by:\n",
            "\n",
            "* **Adding specificity:**  Each option clarifies what aspects of `{topic}` should be addressed.\n",
            "* **Focusing the scope:** Options 2 and 4 narrow the focus to specific aspects or examples.\n",
            "* **Considering target audience:** Option 1 explicitly targets a non-technical audience.\n",
            "* **Specifying desired length/format:** Option 1 sets a word limit, while Option 3 specifies a table format.\n",
            "* **Including example datasets:** Option 4 uses MNIST as a concrete example.\n",
            "\n",
            "\n",
            "They are designed to be more effective in eliciting detailed and relevant responses from an AI, regardless of the specific value of `{topic}`.\n",
            "Iteration 3 prompt: The provided templates are already quite good, but here's a slightly refined version focusing on even greater clarity and flexibility:\n",
            "\n",
            "\n",
            "**Option 1 (Beginner-Friendly):**\n",
            "\n",
            "Explain {topic} to someone with no prior knowledge, using a simple analogy and real-world examples.  Keep it concise (under 500 words).\n",
            "\n",
            "\n",
            "**Option 2 (Process-Oriented):**\n",
            "\n",
            "Describe the process of using or implementing {topic}.  Include key steps, necessary data, common challenges, and how success is measured.\n",
            "\n",
            "\n",
            "**Option 3 (Comparative Analysis):**\n",
            "\n",
            "Compare and contrast at least three distinct methods or approaches related to {topic}.  Use a table to organize your comparison, highlighting key differences and applications.\n",
            "\n",
            "\n",
            "**Option 4 (Advanced/Technical):**\n",
            "\n",
            "Explain the underlying principles and technical details of {topic}.  Include relevant equations, algorithms, or diagrams where applicable.  If relevant, use a specific example dataset or case study to illustrate your explanation.\n",
            "\n",
            "\n",
            "**Improvements:**\n",
            "\n",
            "* **Option 1:**  Replaces \"core principles and potential applications\" with the more accessible \"real-world examples,\" making the prompt easier to understand for someone writing the response.  \"No prior knowledge\" is clearer than \"non-technical audience.\"\n",
            "\n",
            "* **Option 2:**  Replaces the potentially ambiguous \"data requirements, model training, evaluation metrics\" with the more straightforward \"key steps, necessary data, common challenges, and how success is measured.\"  This applies to a wider range of topics, not just machine learning.\n",
            "\n",
            "* **Option 3:**  Maintains the table format but clarifies the need to highlight \"key differences and applications.\"\n",
            "\n",
            "* **Option 4:**  Replaces the less precise \"mathematical foundations, training process, and evaluation metrics\" with \"underlying principles and technical details,\" which is applicable to a broader range of technical topics beyond just machine learning algorithms. The addition of \"If relevant, use a specific example dataset or case study\" adds flexibility and avoids forcing the use of MNIST when inappropriate.\n",
            "\n",
            "\n",
            "These revised templates maintain the strengths of the original while offering improved clarity and broader applicability.  They are designed to elicit more precise and relevant responses from the AI across a wider spectrum of topics represented by `{topic}`.\n",
            "\n",
            "Final refined prompt:\n",
            "The provided templates are already quite good, but here's a slightly refined version focusing on even greater clarity and flexibility:\n",
            "\n",
            "\n",
            "**Option 1 (Beginner-Friendly):**\n",
            "\n",
            "Explain {topic} to someone with no prior knowledge, using a simple analogy and real-world examples.  Keep it concise (under 500 words).\n",
            "\n",
            "\n",
            "**Option 2 (Process-Oriented):**\n",
            "\n",
            "Describe the process of using or implementing {topic}.  Include key steps, necessary data, common challenges, and how success is measured.\n",
            "\n",
            "\n",
            "**Option 3 (Comparative Analysis):**\n",
            "\n",
            "Compare and contrast at least three distinct methods or approaches related to {topic}.  Use a table to organize your comparison, highlighting key differences and applications.\n",
            "\n",
            "\n",
            "**Option 4 (Advanced/Technical):**\n",
            "\n",
            "Explain the underlying principles and technical details of {topic}.  Include relevant equations, algorithms, or diagrams where applicable.  If relevant, use a specific example dataset or case study to illustrate your explanation.\n",
            "\n",
            "\n",
            "**Improvements:**\n",
            "\n",
            "* **Option 1:**  Replaces \"core principles and potential applications\" with the more accessible \"real-world examples,\" making the prompt easier to understand for someone writing the response.  \"No prior knowledge\" is clearer than \"non-technical audience.\"\n",
            "\n",
            "* **Option 2:**  Replaces the potentially ambiguous \"data requirements, model training, evaluation metrics\" with the more straightforward \"key steps, necessary data, common challenges, and how success is measured.\"  This applies to a wider range of topics, not just machine learning.\n",
            "\n",
            "* **Option 3:**  Maintains the table format but clarifies the need to highlight \"key differences and applications.\"\n",
            "\n",
            "* **Option 4:**  Replaces the less precise \"mathematical foundations, training process, and evaluation metrics\" with \"underlying principles and technical details,\" which is applicable to a broader range of technical topics beyond just machine learning algorithms. The addition of \"If relevant, use a specific example dataset or case study\" adds flexibility and avoids forcing the use of MNIST when inappropriate.\n",
            "\n",
            "\n",
            "These revised templates maintain the strengths of the original while offering improved clarity and broader applicability.  They are designed to elicit more precise and relevant responses from the AI across a wider spectrum of topics represented by `{topic}`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Comparing Original and Refined prompt"
      ],
      "metadata": {
        "id": "3ZY3mjtCvVmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_response = generate_response(initial_prompt.format(topic=\"machine learning\"))\n",
        "refined_response = generate_response(refined_prompt.format(topic=\"machine learning\"))\n",
        "\n",
        "original_score = evaluate_response(original_response, criteria)\n",
        "refined_score = evaluate_response(refined_response, criteria)\n",
        "\n",
        "print(f\"Original prompt score: {original_score:.2f}\")\n",
        "print(f\"Refined prompt score: {refined_score:.2f}\")\n",
        "print(f\"Improvement: {(refined_score - original_score):.2f} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi9p9errvI2C",
        "outputId": "a9a283a7-b0e7-4a90-dc0f-925d0f434207"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating response based on clarity...\n",
            "Evaluating response based on informativeness...\n",
            "Evaluating response based on engagement...\n",
            "Evaluating response based on clarity...\n",
            "Evaluating response based on informativeness...\n",
            "Evaluating response based on engagement...\n",
            "Original prompt score: 9.00\n",
            "Refined prompt score: 9.00\n",
            "Improvement: 0.00 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPjTfAJ4vadb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
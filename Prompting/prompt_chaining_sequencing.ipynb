{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt chaining and Sequencing\n",
        "\n",
        "As AI applications become more sophisticated, there's often a need to break down complex tasks into smaller, manageable steps. Prompt chaining and sequencing allow us to guide language models through a series of interrelated prompts, enabling more structured and controlled outputs. This approach is particularly useful for tasks that require multiple stages of processing or decision-making.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Basic Prompt Chaining**: Connecting the output of one prompt to the input of another.\n",
        "2. **Sequential Prompting**: Creating a logical flow of prompts to guide the AI through a multi-step process.\n",
        "3. **Dynamic Prompt Generation**: Using the output of one prompt to dynamically generate the next prompt.\n",
        "4. **Error Handling and Validation**: Implementing checks and balances within the prompt chain."
      ],
      "metadata": {
        "id": "qvI6UtuMaiUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain-google-genai"
      ],
      "metadata": {
        "id": "p9J8lsm9av4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ASrJN_ImZW4h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=''\n",
        "llm= ChatGoogleGenerativeAI(model='gemini-1.5-flash')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Prompt Chaining"
      ],
      "metadata": {
        "id": "0tAukJawbJc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt template\n",
        "story_prompt=PromptTemplate(\n",
        "    input_varibles=['genre'],\n",
        "    template=\"\"\"\n",
        "        Write a short {genre} story in 3-4 sentences.\n",
        "      \"\"\"\n",
        ")\n",
        "\n",
        "summary_prompt=PromptTemplate(\n",
        "    input_variables=['story'],\n",
        "    template='Summarize the following story in one sentence :\\n {story}'\n",
        ")\n",
        "\n",
        "\n",
        "# Chain the prompt\n",
        "def story_chain(genre):\n",
        "    \"\"\"Generate a story and its summary based on a given genre.\n",
        "\n",
        "    Args:\n",
        "        genre (str): The genre of the story to generate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the generated story and its summary.\n",
        "    \"\"\"\n",
        "    story=(story_prompt | llm).invoke({'genre':genre}).content\n",
        "    summary=(summary_prompt | llm).invoke({'story':story}).content\n",
        "    return story,summary\n",
        "\n",
        "# Test the chain\n",
        "genre='Science Friction'\n",
        "story,summary=story_chain(genre)\n",
        "print(f\"Story: {story}\\n\\n summary:{summary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38PyaMVba8LI",
        "outputId": "238f7366-a731-4fc5-f195-52bf250e3e6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story: The shimmering portal spat out Dr. Aris Thorne, covered in iridescent slime and smelling faintly of ozone.  His experimental wormhole generator had worked, but the destination wasn't the Andromeda galaxy; it was a fungal planet teeming with sentient, bioluminescent mushrooms who demanded tribute in the form of perfectly ripe mangoes.  Aris sighed; paperwork was going to be a nightmare.\n",
            "\n",
            " summary:After a mishap with his wormhole generator, Dr. Thorne found himself on a fungal planet populated by sentient mushrooms who demanded mangoes as tribute.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Prompting"
      ],
      "metadata": {
        "id": "fivQC0X_dAki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt templates for each step\n",
        "theme_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Identify the main theme of the following text:\\n{text}\"\n",
        ")\n",
        "\n",
        "tone_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Describe the overall tone of the following text:\\n{text}\"\n",
        ")\n",
        "\n",
        "takeaway_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"theme\", \"tone\"],\n",
        "    template=\"Given the following text with the main theme '{theme}' and tone '{tone}', what are the key takeaways?\\n{text}\"\n",
        ")\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"Perform a multi-step analysis of a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the theme, tone, and key takeaways of the text.\n",
        "    \"\"\"\n",
        "    theme = (theme_prompt | llm).invoke({\"text\": text}).content\n",
        "    tone = (tone_prompt | llm).invoke({\"text\": text}).content\n",
        "    takeaways = (takeaway_prompt | llm).invoke({\"text\": text, \"theme\": theme, \"tone\": tone}).content\n",
        "    return {\"theme\": theme, \"tone\": tone, \"takeaways\": takeaways}\n",
        "\n",
        "# Test the sequential prompting\n",
        "sample_text = \"The rapid advancement of artificial intelligence has sparked both excitement and concern among experts. While AI promises to revolutionize industries and improve our daily lives, it also raises ethical questions about privacy, job displacement, and the potential for misuse. As we stand on the brink of this technological revolution, it's crucial that we approach AI development with caution and foresight, ensuring that its benefits are maximized while its risks are minimized.\"\n",
        "\n",
        "analysis = analyze_text(sample_text)\n",
        "for key, value in analysis.items():\n",
        "    print(f\"{key.capitalize()}: {value}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msr-S4HucrjP",
        "outputId": "c4390a8f-1ce6-420c-adbe-be42d05dac92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theme: The main theme is the **ethical considerations and responsible development of artificial intelligence**.  The text highlights both the potential benefits and the significant risks associated with AI, emphasizing the need for careful planning and mitigation of negative consequences.\n",
            "\n",
            "Tone: The overall tone of the text is **cautiously optimistic**.  It acknowledges the immense potential of AI (\"revolutionize industries,\" \"improve our daily lives\") while simultaneously highlighting the serious risks and ethical concerns (\"ethical questions,\" \"job displacement,\" \"potential for misuse\").  The call for \"caution and foresight\" underscores this balanced and measured perspective.\n",
            "\n",
            "Takeaways: The key takeaways from the text are:\n",
            "\n",
            "* **AI's dual nature:** AI offers immense potential benefits (revolutionizing industries, improving daily lives), but also presents significant risks (privacy violations, job displacement, misuse).\n",
            "\n",
            "* **Ethical considerations are paramount:**  The development and deployment of AI must prioritize ethical considerations to mitigate potential harm.  This includes addressing concerns about privacy and job displacement.\n",
            "\n",
            "* **Responsible development is crucial:**  A cautious and proactive approach to AI development is necessary.  This involves careful planning and the implementation of strategies to minimize risks while maximizing benefits.  Foresight and responsible innovation are key.\n",
            "\n",
            "* **Balanced perspective is needed:**  While acknowledging the exciting potential of AI, a realistic and measured approach is needed that considers both the positive and negative aspects.  Unbridled enthusiasm should be tempered with careful consideration of the ethical and societal implications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Prompting"
      ],
      "metadata": {
        "id": "znzt_DAghB2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt templates\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question concisely:\\n{question}\"\n",
        ")\n",
        "\n",
        "follow_up_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"Based on the question '{question}' and the answer '{answer}', generate a relevant follow-up question.\"\n",
        ")\n",
        "\n",
        "def dynamic_qa(initial_question, num_follow_ups=3):\n",
        "    \"\"\"Conduct a dynamic Q&A session with follow-up questions.\n",
        "\n",
        "    Args:\n",
        "        initial_question (str): The initial question to start the Q&A session.\n",
        "        num_follow_ups (int): The number of follow-up questions to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing questions and answers.\n",
        "    \"\"\"\n",
        "    qa_chain = []\n",
        "    current_question = initial_question\n",
        "\n",
        "    for _ in range(num_follow_ups + 1):  # +1 for the initial question\n",
        "        answer = (answer_prompt | llm).invoke({\"question\": current_question}).content\n",
        "        qa_chain.append({\"question\": current_question, \"answer\": answer})\n",
        "\n",
        "        if _ < num_follow_ups:  # Generate follow-up for all but the last iteration\n",
        "            current_question = (follow_up_prompt | llm).invoke({\"question\": current_question, \"answer\": answer}).content\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "# Test the dynamic Q&A system\n",
        "initial_question = \"What are the potential applications of quantum computing?\"\n",
        "qa_session = dynamic_qa(initial_question)\n",
        "\n",
        "for i, qa in enumerate(qa_session):\n",
        "    print(f\"Q{i+1}: {qa['question']}\")\n",
        "    print(f\"A{i+1}: {qa['answer']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIFotHFMdDYL",
        "outputId": "584fd98b-83f2-4415-ffed-2605c3f0e832"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: What are the potential applications of quantum computing?\n",
            "A1: Drug discovery, materials science, financial modeling, cryptography, optimization problems, and artificial intelligence.\n",
            "\n",
            "Q2: What are the biggest challenges currently hindering the widespread adoption and practical application of quantum computing in these fields?\n",
            "A2: High cost, qubit instability, error correction limitations, and lack of readily available algorithms and skilled workforce.\n",
            "\n",
            "Q3: Given the challenges of high cost, qubit instability, error correction limitations, and lack of skilled workforce, what are the most promising short-term and long-term strategies for overcoming these obstacles and accelerating the adoption of quantum computing?\n",
            "A3: **Short-term:** Focus on hybrid quantum-classical algorithms, improved qubit coherence, and developing accessible quantum computing platforms & training programs.\n",
            "\n",
            "**Long-term:**  Invest in fault-tolerant quantum computing architectures, scalable fabrication techniques, and robust error correction codes, alongside fostering a diverse and skilled quantum workforce through education and research.\n",
            "\n",
            "Q4: Given the significant investment required in both short-term and long-term strategies for quantum computing development, how can we best prioritize funding and resource allocation to maximize the return on investment and ensure the field's sustainable growth?\n",
            "A4: Prioritize funding based on demonstrable progress towards fault-tolerant quantum computers, focusing on  interconnected efforts in hardware, software, and algorithm development, and fostering collaboration between academia and industry.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Handeling and Validation"
      ],
      "metadata": {
        "id": "GTxKh-mXhGXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt templates\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Generate a 4-digit number related to the topic: {topic}. Respond with ONLY the number, no additional text.\"\n",
        ")\n",
        "\n",
        "validate_prompt = PromptTemplate(\n",
        "    input_variables=[\"number\", \"topic\"],\n",
        "    template=\"Is the number {number} truly related to the topic '{topic}'? Answer with 'Yes' or 'No' and explain why.\"\n",
        ")\n",
        "\n",
        "def extract_number(text):\n",
        "    \"\"\"Extract a 4-digit number from the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to extract the number from.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The extracted 4-digit number, or None if no valid number is found.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\b\\d{4}\\b', text)\n",
        "    return match.group() if match else None\n",
        "\n",
        "def robust_number_generation(topic, max_attempts=3):\n",
        "    \"\"\"Generate a topic-related number with validation and error handling.\n",
        "\n",
        "    Args:\n",
        "        topic (str): The topic to generate a number for.\n",
        "        max_attempts (int): Maximum number of generation attempts.\n",
        "\n",
        "    Returns:\n",
        "        str: A validated 4-digit number or an error message.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = (generate_prompt | llm).invoke({\"topic\": topic}).content\n",
        "            number = extract_number(response)\n",
        "\n",
        "            if not number:\n",
        "                raise ValueError(f\"Failed to extract a 4-digit number from the response: {response}\")\n",
        "\n",
        "            # Validate the relevance\n",
        "            validation = (validate_prompt | llm).invoke({\"number\": number, \"topic\": topic}).content\n",
        "            if validation.lower().startswith(\"yes\"):\n",
        "                return number\n",
        "            else:\n",
        "                print(f\"Attempt {attempt + 1}: Number {number} was not validated. Reason: {validation}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "\n",
        "    return \"Failed to generate a valid number after multiple attempts.\"\n",
        "\n",
        "# Test the robust number generation\n",
        "topic = \"World War II\"\n",
        "result = robust_number_generation(topic)\n",
        "print(f\"Final result for topic '{topic}': {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvKkvSvXhDbp",
        "outputId": "784aa45b-2383-454e-835c-00834e62e4b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final result for topic 'World War II': 1945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjCcqDdUhJnx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
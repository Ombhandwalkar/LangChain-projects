import torch 
import wandb
from dotenv import load_dotenv
load_dotenv()
from datasets import load_dataset
from tqdm import tqdm
from transformers import pipeline, AutoTokenizer
# Transfomer Reinforcement Learning to apply PPO
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler

model_name='lvwerra/gpt2-imdb'

# Prepare dataset to be used in PPO training
def build_dataset(config, dataset_name='imdb',input_min_text_length=2, input_max_text_length=8):
    """Build a dataset to be used for training.
       It is a series of prompts, each with different length chosen randomly.
       We will use it to generate the response and compute the rewards    
    """
    tokenizer= AutoTokenizer.from_pretrained(model_name)
    # Padding the prompt to end of sentence
    tokenizer.pad_token = tokenizer.eos_token
    # Load IMDB dataset
    ds= load_dataset(dataset_name, split='train')
    ds= ds.rename_column('text','review')
    # Only choose reviews with more than 200 tokens
    ds =ds.filter(lambda x: len(x['review'])> 200, batched=False)

    # Truncate  each review randomly to 2-8 tokens as prompt
    input_size= LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        # From each review just keep first 'input_size' tokens, this represents the prompt used to generate the response.
        sample['input_ids'] = tokenizer.encode(sample['review'])[:input_size()]
        sample['query'] = tokenizer.decode(sample['input_ids'])
        return sample

    ds= ds.map(tokenize, batched=False)
    ds.set_format(type='torch')
    return ds

# Gather a batch of fields into list
def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# Initialize PPO configuration 
if __name__=='__main__':
    config= PPOConfig(
        learning_rate=1.41e-5,
        push_to_hub=True,
        report_to='wandb'
        
    )

    wandb.init()

    dataset = build_dataset(config)

    # This model we are going to fine-tune with PPO
    model= AutoModelForCausalLMWithValueHead.from_pretrained(model_name)
    # This is reference model(froze) for KL divergence
    ref_model= AutoModelForCausalLMWithValueHead.from_pretrained(model_name)

    tokenizer =AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token= tokenizer.eos_token

    # Initialize  PPO trainer
    # It handles Forward pass, reward collection, log probability computations, PPO optimization steps
    ppo_trainer= PPOTrainer(config, model,ref_model, dataset,  data_collator=collator)

    ppo_trainer.tokenizer=tokenizer
    device= ppo_trainer(dataset)
    if ppo_trainer.accelerator.num_processes==1:
        device =0 if torch.cuda.is_available() else 'cpu'

    # This is the reward model 
    sentiment_pipe= pipeline('sentiment-analysis',model='lvwerra/distilbert-imdb',device=device)

    # Pring some examples of sentiments generated by reward model
    sent_kwargs={'return_all_scores':True, 'function_to_apply':'none','batch_size':16}
    text = 'This movie was really bad !'
    print(sentiment_pipe(text, **sent_kwargs))

    text= 'This movie was really good !'
    print(sentiment_pipe(text,**sent_kwargs))

    output_min_length=4
    output_max_length=16
    output_length_sampler = LengthSampler(output_min_length,output_max_length)

    # This configuration to generate responses [Trajectories]
    response_generation_kwargs={
        'min_length':-1,
        'top_k':0.0,
        'top_p':1.0,
        'do_sample':True,
        'pad_token_id':tokenizer.eos_token_id
    }

    # Training loop with PPO
    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
        query_tensors = batch['input_ids']

        ## Phase-1 : Get trajectories from offline policy
        # In this case we are only generating the response, but not computing the log probabilities, which will be computed internally by the PPOTrainer.
        response_tensors=[]
        # Generate responses from model
        for query in query_tensors:
            gen_len =output_length_sampler()
            response_generation_kwargs['max_new_tokens'] = gen_len # Number of tokens to generate (chosen randomly)
            response = ppo_trainer.generate(query, **response_generation_kwargs) # This returns the (query + response) tokens
            response_tensors.append(response.squeeze()[-gen_len:]) #  Only take the tokens corresponding to the genearted response [ Remove the prompt/query from begining]
        batch['response'] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

        ## Phase-2 : Compute rewards
        # Joing the query (pormpt) + response (generated tokens)
        texts = [q + r for q,r in zip(batch['query'], batch['response'])]
        # Compute the rewards for each of the texts (query + response)
        # shape: A list of dictionaries with two keys: POSITIVE and NEGATIVE. We are interested in the POSITIVE score. This will be our reward.
        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

        # The reward for each text is the score (logits) corresponding to POSITIVE class.
        # Shape : A list of scalers, one for each generated response.
        # This means we assign  the reward to the whole response, not to each token.
        rewards = [torch.tensor(output[1]['score']) for output in pipe_outputs]

        ## Phase-1 + Phase-2 : Calculate the logprobs and then run PPO update
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)

        ppo_trainer.log_stats(stats, batch, rewards)

model.save_pretrained('gpt2-imdb-v2',push_to_hub=False)
tokenizer.save_pretrained('gpt2-imdb-pos-v2',push_to_hub=False)
model.push_to_hub('OmBhandwalkar/gpt2-imdb-PPO',use_auth_token=True)
tokenizer.push_to_hub('OmBhandwalkar/gpt2-imdb-PPO-v2',push_to_hub=True)